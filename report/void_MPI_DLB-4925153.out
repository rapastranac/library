
Lmod is automatically replacing "intel/2020.1.217" with "gcc/10.2.0".


Inactive Modules:
  1) libfabric/1.10.1     2) openmpi/4.0.3     3) ucx/1.8.0

The following have been reloaded with a version change:
  1) gcccore/.9.3.0 => gcccore/.10.2.0


Activating Modules:
  1) libfabric/1.10.1     2) openmpi/4.0.5     3) ucx/1.9.0


Currently Loaded Modules:
  1) CCEnv           (S)      5) StdEnv/2020     (S)   9) pmix/3.1.5
  2) CCconfig                 6) gcccore/.10.2.0 (H)  10) libfabric/1.10.1
  3) gentoo/2020     (S)      7) gcc/10.2.0      (t)  11) openmpi/4.0.5    (m)
  4) imkl/2020.1.217 (math)   8) ucx/1.9.0

  Where:
   H:     Hidden Module
   S:     Module is Sticky, requires --force to unload or purge
   m:     MPI implementations / Implémentations MPI
   math:  Mathematical libraries / Bibliothèques mathématiques
   t:     Tools for development / Outils de développement

 

Current working directory: /gpfs/fs0/scratch/m/mlafond/pasr1602/library
Starting run at: Tue Mar  2 18:38:59 EST 2021
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 0 of 20 is on nia0097.scinet.local
About to start, 0 / 20!! 
scheduler() launched!! 
buffer sucessfully sent! 
*** Busy nodes: 1 ***
 Scheduler started!! 
BusyNodes = 0 achieved 
Center received a best result from 4, Bytes : 2152, refVal 265 
Center received a best result from 5, Bytes : 2172, refVal 270 
Center received a best result from 6, Bytes : 2192, refVal 275 
Center received a best result from 10, Bytes : 2168, refVal 269 
process 0 waiting at barrier 
process 0 passed barrier 

 
 
*****************************************************
Elapsed time : 264.717 
Total number of requests : 306320 
Number of approved requests : 24099 
Number of failed requests : 282221 
*****************************************************

 
 
Stream retrieved, size : 2152 
Cover size : 328 

Global pool idle time: 1044.241625 seconds


rank 0, before deallocate 
rank 0, after deallocate 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 19 of 20 is on nia1427.scinet.local
About to start, 19 / 20!! 
rank 19 about to send best result to center 
rank 19 did not catch a best result 
Exit tag received on process 19 
process 19 waiting at barrier 
process 19 passed barrier 
rank 19, before deallocate 
rank 19, after deallocate 
rank 19, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 16 of 20 is on nia1390.scinet.local
About to start, 16 / 20!! 
rank 16 about to send best result to center 
rank 16 did not catch a best result 
Exit tag received on process 16 
process 16 waiting at barrier 
process 16 passed barrier 
rank 16, before deallocate 
rank 16, after deallocate 
rank 16, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 17 of 20 is on nia1391.scinet.local
About to start, 17 / 20!! 
rank 17 about to send best result to center 
rank 17 did not catch a best result 
Exit tag received on process 17 
process 17 waiting at barrier 
process 17 passed barrier 
rank 17, before deallocate 
rank 17, after deallocate 
rank 17, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 5 of 20 is on nia0339.scinet.local
About to start, 5 / 20!! 
MVC found so far has 270 elements..................thread 19
rank 5 updated refValueGlobalAbsolute to 270 || 270 
rank 5, cover size : 270 
rank 5, buffer size to be sent : 2172 
rank 5 about to send best result to center 
rank 5 put signal in inbox to retrieve a best result 
rank 5 sent best result, Bytes : 2172, refVal : 270
Exit tag received on process 5 
process 5 waiting at barrier 
process 5 passed barrier 
rank 5, before deallocate 
rank 5, after deallocate 
rank 5, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 10 of 20 is on nia0855.scinet.local
About to start, 10 / 20!! 
MVC found so far has 273 elements...................thread 1
rank 10 updated refValueGlobalAbsolute to 273 || 273 
rank 10, cover size : 273 
rank 10, buffer size to be sent : 2184 
MVC found so far has 272 elements...................thread 2
rank 10 updated refValueGlobalAbsolute to 272 || 272 
rank 10, cover size : 272 
rank 10, buffer size to be sent : 2180 
MVC found so far has 271 elements...................thread 4
rank 10 updated refValueGlobalAbsolute to 271 || 271 
rank 10, cover size : 271 
rank 10, buffer size to be sent : 2176 
MVC found so far has 269 elements...................thread 5
rank 10 updated refValueGlobalAbsolute to 269 || 269 
rank 10, cover size : 269 
rank 10, buffer size to be sent : 2168 
rank 10 about to send best result to center 
rank 10 put signal in inbox to retrieve a best result 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 9 of 20 is on nia0826.scinet.local
About to start, 9 / 20!! 
rank 9 about to send best result to center 
rank 9 did not catch a best result 
Exit tag received on process 9 
process 9 waiting at barrier 
process 9 passed barrier 
rank 9, before deallocate 
rank 9, after deallocate 
rank 9, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 12 of 20 is on nia1018.scinet.local
About to start, 12 / 20!! 
rank 12 about to send best result to center 
rank 12 did not catch a best result 
Exit tag received on process 12 
process 12 waiting at barrier 
process 12 passed barrier 
rank 12, before deallocate 
rank 12, after deallocate 
rank 12, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 3 of 20 is on nia0237.scinet.local
About to start, 3 / 20!! 
rank 3 about to send best result to center 
rank 3 did not catch a best result 
Exit tag received on process 3 
process 3 waiting at barrier 
process 3 passed barrier 
rank 3, before deallocate 
rank 3, after deallocate 
rank 3, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 14 of 20 is on nia1216.scinet.local
About to start, 14 / 20!! 
rank 14 about to send best result to center 
rank 14 did not catch a best result 
Exit tag received on process 14 
process 14 waiting at barrier 
process 14 passed barrier 
rank 14, before deallocate 
rank 14, after deallocate 
rank 14, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 15 of 20 is on nia1218.scinet.local
About to start, 15 / 20!! 
rank 15 about to send best result to center 
rank 15 did not catch a best result 
Exit tag received on process 15 
process 15 waiting at barrier 
process 15 passed barrier 
rank 15, before deallocate 
rank 15, after deallocate 
rank 15, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 2 of 20 is on nia0236.scinet.local
About to start, 2 / 20!! 
rank 2 about to send best result to center 
rank 2 did not catch a best result 
Exit tag received on process 2 
process 2 waiting at barrier 
process 2 passed barrier 
rank 2, before deallocate 
rank 2, after deallocate 
rank 2, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 8 of 20 is on nia0696.scinet.local
About to start, 8 / 20!! 
rank 8 about to send best result to center 
rank 8 did not catch a best result 
Exit tag received on process 8 
process 8 waiting at barrier 
process 8 passed barrier 
rank 8, before deallocate 
rank 8, after deallocate 
rank 8, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 7 of 20 is on nia0402.scinet.local
About to start, 7 / 20!! 
rank 7 about to send best result to center 
rank 7 did not catch a best result 
Exit tag received on process 7 
process 7 waiting at barrier 
process 7 passed barrier 
rank 7, before deallocate 
rank 7, after deallocate 
rank 7, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 6 of 20 is on nia0397.scinet.local
About to start, 6 / 20!! 
MVC found so far has 281 elements...................thread 0
rank 6 updated refValueGlobalAbsolute to 281 || 281 
rank 6, cover size : 281 
rank 6, buffer size to be sent : 2216 
MVC found so far has 280 elements..................thread 29
rank 6 updated refValueGlobalAbsolute to 280 || 280 
rank 6, cover size : 280 
rank 6, buffer size to be sent : 2212 
MVC found so far has 279 elements...................thread 4
rank 6 updated refValueGlobalAbsolute to 279 || 279 
rank 6, cover size : 279 
rank 6, buffer size to be sent : 2208 
MVC found so far has 278 elements..................thread 16
rank 6 updated refValueGlobalAbsolute to 278 || 278 
rank 6, cover size : 278 
rank 6, buffer size to be sent : 2204 
MVC found so far has 277 elements..................thread 19
rank 6 updated refValueGlobalAbsolute to 277 || 277 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 13 of 20 is on nia1019.scinet.local
About to start, 13 / 20!! 
rank 13 about to send best result to center 
rank 13 did not catch a best result 
Exit tag received on process 13 
process 13 waiting at barrier 
process 13 passed barrier 
rank 13, before deallocate 
rank 13, after deallocate 
rank 13, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 4 of 20 is on nia0299.scinet.local
About to start, 4 / 20!! 
MVC found so far has 268 elements..................thread 27
rank 4 updated refValueGlobalAbsolute to 268 || 268 
rank 4, cover size : 268 
rank 4, buffer size to be sent : 2164 
MVC found so far has 267 elements..................thread 22
rank 4 updated refValueGlobalAbsolute to 267 || 267 
rank 4, cover size : 267 
rank 4, buffer size to be sent : 2160 
MVC found so far has 266 elements..................thread 26
rank 4 updated refValueGlobalAbsolute to 266 || 266 
rank 4, cover size : 266 
rank 4, buffer size to be sent : 2156 
MVC found so far has 265 elements..................thread 30
rank 4 updated refValueGlobalAbsolute to 265 || 265 
rank 4, cover size : 265 
rank 4, buffer size to be sent : 2152 
rank 4 about to send best result to center 
rank 4 put signal in inbox to retrieve a best result 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 11 of 20 is on nia0878.scinet.local
About to start, 11 / 20!! 
rank 11 about to send best result to center 
rank 11 did not catch a best result 
Exit tag received on process 11 
process 11 waiting at barrier 
process 11 passed barrier 
rank 11, before deallocate 
rank 11, after deallocate 
rank 11, after MPI_Finalize() 
rank 0, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 1 of 20 is on nia0098.scinet.local
About to start, 1 / 20!! 
rank 1 about to send best result to center 
rank 1 did not catch a best result 
Exit tag received on process 1 
process 1 waiting at barrier 
process 1 passed barrier 
rank 1, before deallocate 
rank 1, after deallocate 
rank 1, after MPI_Finalize() 
rank 6, cover size : 277 
rank 6, buffer size to be sent : 2200 
MVC found so far has 276 elements..................thread 21
rank 6 updated refValueGlobalAbsolute to 276 || 276 
rank 6, cover size : 276 
rank 6, buffer size to be sent : 2196 
MVC found so far has 275 elements..................thread 15
rank 6 updated refValueGlobalAbsolute to 275 || 275 
rank 6, cover size : 275 
rank 6, buffer size to be sent : 2192 
rank 6 about to send best result to center 
rank 6 put signal in inbox to retrieve a best result 
rank 6 sent best result, Bytes : 2192, refVal : 275
Exit tag received on process 6 
process 6 waiting at barrier 
process 6 passed barrier 
rank 6, before deallocate 
rank 6, after deallocate 
rank 6, after MPI_Finalize() 
rank 10 sent best result, Bytes : 2168, refVal : 269
Exit tag received on process 10 
process 10 waiting at barrier 
process 10 passed barrier 
rank 10, before deallocate 
rank 10, after deallocate 
rank 10, after MPI_Finalize() 
rank 4 sent best result, Bytes : 2152, refVal : 265
Exit tag received on process 4 
process 4 waiting at barrier 
process 4 passed barrier 
rank 4, before deallocate 
rank 4, after deallocate 
rank 4, after MPI_Finalize() 
argc: 3, threads: 40, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 18 of 20 is on nia1426.scinet.local
About to start, 18 / 20!! 
rank 18 about to send best result to center 
rank 18 did not catch a best result 
Exit tag received on process 18 
process 18 waiting at barrier 
process 18 passed barrier 
rank 18, before deallocate 
rank 18, after deallocate 
rank 18, after MPI_Finalize() 
Finishing run at: Tue Mar  2 18:43:33 EST 2021

scontrol show jobid 4925153
JobId=4925153 JobName=void_MPI_DLB
   UserId=pasr1602(3102120) GroupId=mlafond(6054778) MCS_label=N/A
   Priority=2211680 Nice=0 Account=def-mlafond QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:04:50 TimeLimit=02:00:00 TimeMin=N/A
   SubmitTime=2021-03-02T18:25:21 EligibleTime=2021-03-02T18:25:21
   AccrueTime=2021-03-02T18:25:21
   StartTime=2021-03-02T18:38:43 EndTime=2021-03-02T18:43:33 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-03-02T18:38:43
   Partition=compute AllocNode:Sid=nia-login01:159731
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia[0097-0098,0236-0237,0299,0339,0397,0402,0696,0826,0855,0878,1018-1019,1216,1218,1390-1391,1426-1427]
   BatchHost=nia0097
   NumNodes=20 NumCPUs=1600 NumTasks=20 CPUs/Task=40 ReqB:S:C:T=0:0:*:*
   TRES=cpu=1600,mem=3500G,node=20,billing=800
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=40 MinMemoryNode=175G MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/m/mlafond/pasr1602/library/job.sh
   WorkDir=/gpfs/fs0/scratch/m/mlafond/pasr1602/library
   StdErr=/gpfs/fs0/scratch/m/mlafond/pasr1602/library/report/void_MPI_DLB-4925153.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/m/mlafond/pasr1602/library/report/void_MPI_DLB-4925153.out
   Power=
   MailUser=pasr1602@usherbrooke.ca MailType=BEGIN,END,FAIL,REQUEUE

sacct -j 4925153
       JobID    JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
4925153      void_MPI_+ def-mlafo+   00:04:50                        38:42.411 1-19:40:11      0:0 
4925153.bat+      batch def-mlafo+   00:04:50   1723044K     10180K  00:01.562  00:03.503      0:0 
4925153.ext+     extern def-mlafo+   00:04:50    138360K      1068K  00:00.006  00:00.010      0:0 
4925153.0         a.out def-mlafo+   00:04:34   3237448K    421640K  38:40.843 1-19:40:08      0:0 

kernel messages produced during job executions:
[Mar 2 18:32] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff45
[  +0.011071] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff46
[  +0.011026] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff47
[  +0.010998] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff48
[  +0.010987] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff49
[  +0.010979] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff4a
[  +0.010971] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff4b
[  +0.010964] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff4c
[  +0.010962] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff4d
[  +0.010958] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff4e
[  +0.010944] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff4f
[  +0.010952] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff50
[  +0.010954] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff51
[  +0.010956] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff52
[  +0.010949] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff53
[  +0.010943] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff54
[  +0.010940] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff55
[  +0.010922] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff56
[  +0.010926] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff57
[  +0.010933] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff58
[  +0.273997] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff31
[  +0.010953] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff32
[  +0.010932] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff33
[  +0.010924] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff34
[  +0.010921] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff35
[  +0.010922] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff36
[  +0.010924] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff37
[  +0.010923] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff38
[  +0.010925] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff39
[  +0.010926] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff59
[  +0.010913] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff5a
[  +0.010901] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff5b
[  +0.010878] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff5c
[  +0.010868] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff5d
[  +0.010859] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff5e
[  +0.010850] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff5f
[  +0.010844] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff60
[  +0.010825] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff61
[  +0.010803] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff62
[  +0.010785] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0xff63
