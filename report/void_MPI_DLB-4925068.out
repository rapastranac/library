
Lmod is automatically replacing "intel/2020.1.217" with "gcc/10.2.0".


Inactive Modules:
  1) libfabric/1.10.1     2) openmpi/4.0.3     3) ucx/1.8.0

The following have been reloaded with a version change:
  1) gcccore/.9.3.0 => gcccore/.10.2.0


Activating Modules:
  1) libfabric/1.10.1     2) openmpi/4.0.5     3) ucx/1.9.0


Currently Loaded Modules:
  1) CCEnv           (S)      5) StdEnv/2020     (S)   9) pmix/3.1.5
  2) CCconfig                 6) gcccore/.10.2.0 (H)  10) libfabric/1.10.1
  3) gentoo/2020     (S)      7) gcc/10.2.0      (t)  11) openmpi/4.0.5    (m)
  4) imkl/2020.1.217 (math)   8) ucx/1.9.0

  Where:
   H:     Hidden Module
   S:     Module is Sticky, requires --force to unload or purge
   m:     MPI implementations / Implémentations MPI
   math:  Mathematical libraries / Bibliothèques mathématiques
   t:     Tools for development / Outils de développement

 

Current working directory: /gpfs/fs0/scratch/m/mlafond/pasr1602/library
Starting run at: Tue Mar  2 18:04:31 EST 2021
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 13 of 20 is on nia0756.scinet.local
About to start, 13 / 20!! 
rank 13 about to send best result to center 
rank 13 did not catch a best result 
Exit tag received on process 13 
process 13 waiting at barrier 
process 13 passed barrier 
rank 13, before deallocate 
rank 13, after deallocate 
rank 13, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 8 of 20 is on nia0511.scinet.local
About to start, 8 / 20!! 
rank 8 about to send best result to center 
rank 8 did not catch a best result 
Exit tag received on process 8 
process 8 waiting at barrier 
process 8 passed barrier 
rank 8, before deallocate 
rank 8, after deallocate 
rank 8, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 10 of 20 is on nia0552.scinet.local
About to start, 10 / 20!! 
rank 10 about to send best result to center 
rank 10 did not catch a best result 
Exit tag received on process 10 
process 10 waiting at barrier 
process 10 passed barrier 
rank 10, before deallocate 
rank 10, after deallocate 
rank 10, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 19 of 20 is on nia1177.scinet.local
About to start, 19 / 20!! 
rank 19 about to send best result to center 
rank 19 did not catch a best result 
Exit tag received on process 19 
process 19 waiting at barrier 
process 19 passed barrier 
rank 19, before deallocate 
rank 19, after deallocate 
rank 19, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 0 of 20 is on nia0048.scinet.local
About to start, 0 / 20!! 
scheduler() launched!! 
buffer sucessfully sent! 
*** Busy nodes: 1 ***
 Scheduler started!! 
BusyNodes = 0 achieved 
Center received a best result from 4, Bytes : 2188, refVal 274 
Center received a best result from 14, Bytes : 2208, refVal 279 
Center received a best result from 16, Bytes : 2172, refVal 270 
Center received a best result from 17, Bytes : 2152, refVal 265 
Center received a best result from 18, Bytes : 2216, refVal 281 
process 0 waiting at barrier 
process 0 passed barrier 

 
 
*****************************************************
Elapsed time : 646.395 
Total number of requests : 423054 
Number of approved requests : 29221 
Number of failed requests : 393833 
*****************************************************

 
 
Stream retrieved, size : 2152 
Cover size : 328 

argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 4 of 20 is on nia0228.scinet.local
About to start, 4 / 20!! 
MVC found so far has 276 elements...................thread 8
rank 4 updated refValueGlobalAbsolute to 276 || 276 
rank 4, cover size : 276 
rank 4, buffer size to be sent : 2196 
MVC found so far has 274 elements...................thread 2
rank 4 updated refValueGlobalAbsolute to 274 || 274 
rank 4, cover size : 274 
rank 4, buffer size to be sent : 2188 
rank 4 about to send best result to center 
rank 4 put signal in inbox to retrieve a best result 
rank 4 sent best result, Bytes : 2188, refVal : 274
Exit tag received on process 4 
process 4 waiting at barrier 
process 4 passed barrier 
rank 4, before deallocate 
rank 4, after deallocate 
rank 4, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 6 of 20 is on nia0284.scinet.local
About to start, 6 / 20!! 
rank 6 about to send best result to center 
rank 6 did not catch a best result 
Exit tag received on process 6 
process 6 waiting at barrier 
process 6 passed barrier 
rank 6, before deallocate 
rank 6, after deallocate 
rank 6, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 2 of 20 is on nia0049.scinet.local
About to start, 2 / 20!! 
rank 2 about to send best result to center 
rank 2 did not catch a best result 
Exit tag received on process 2 
process 2 waiting at barrier 
process 2 passed barrier 
rank 2, before deallocate 
rank 2, after deallocate 
rank 2, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 14 of 20 is on nia0784.scinet.local
About to start, 14 / 20!! 
MVC found so far has 279 elements...................thread 1
rank 14 updated refValueGlobalAbsolute to 279 || 279 
rank 14, cover size : 279 
rank 14, buffer size to be sent : 2208 
rank 14 about to send best result to center 
rank 14 put signal in inbox to retrieve a best result 
rank 14 sent best result, Bytes : 2208, refVal : 279
Exit tag received on process 14 
process 14 waiting at barrier 
process 14 passed barrier 
rank 14, before deallocate 
rank 14, after deallocate 
rank 14, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 17 of 20 is on nia1176.scinet.local
About to start, 17 / 20!! 
MVC found so far has 269 elements...................thread 0
rank 17 updated refValueGlobalAbsolute to 269 || 269 
rank 17, cover size : 269 
rank 17, buffer size to be sent : 2168 
MVC found so far has 268 elements..................thread 12
rank 17 updated refValueGlobalAbsolute to 268 || 268 
rank 17, cover size : 268 
rank 17, buffer size to be sent : 2164 
MVC found so far has 266 elements...................thread 6
rank 17 updated refValueGlobalAbsolute to 266 || 266 
rank 17, cover size : 266 
rank 17, buffer size to be sent : 2156 
MVC found so far has 265 elements..................thread 10
rank 17 updated refValueGlobalAbsolute to 265 || 265 
rank 17, cover size : 265 
rank 17, buffer size to be sent : 2152 
rank 17 about to send best result to center 
rank 17 put signal in inbox to retrieve a best result 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 12 of 20 is on nia0756.scinet.local
About to start, 12 / 20!! 
rank 12 about to send best result to center 
rank 12 did not catch a best result 
Exit tag received on process 12 
process 12 waiting at barrier 
process 12 passed barrier 
rank 12, before deallocate 
rank 12, after deallocate 
rank 12, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 9 of 20 is on nia0511.scinet.local
About to start, 9 / 20!! 
rank 9 about to send best result to center 
rank 9 did not catch a best result 
Exit tag received on process 9 
process 9 waiting at barrier 
process 9 passed barrier 
rank 9, before deallocate 
rank 9, after deallocate 
rank 9, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 18 of 20 is on nia1177.scinet.local
About to start, 18 / 20!! 
MVC found so far has 281 elements...................thread 4
rank 18 updated refValueGlobalAbsolute to 281 || 281 
rank 18, cover size : 281 
rank 18, buffer size to be sent : 2216 
rank 18 about to send best result to center 
rank 18 put signal in inbox to retrieve a best result 
rank 18 sent best result, Bytes : 2216, refVal : 281
Exit tag received on process 18 
process 18 waiting at barrier 
process 18 passed barrier 
rank 18, before deallocate 
rank 18, after deallocate 
rank 18, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 11 of 20 is on nia0552.scinet.local
About to start, 11 / 20!! 
rank 11 about to send best result to center 
rank 11 did not catch a best result 
Exit tag received on process 11 
process 11 waiting at barrier 
process 11 passed barrier 
rank 11, before deallocate 
rank 11, after deallocate 
rank 11, after MPI_Finalize() 
Global pool idle time: 4185.265974 seconds


rank 0, before deallocate 
rank 0, after deallocate 
rank 0, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 1 of 20 is on nia0048.scinet.local
About to start, 1 / 20!! 
rank 1 about to send best result to center 
rank 1 did not catch a best result 
Exit tag received on process 1 
process 1 waiting at barrier 
process 1 passed barrier 
rank 1, before deallocate 
rank 1, after deallocate 
rank 1, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 5 of 20 is on nia0228.scinet.local
About to start, 5 / 20!! 
rank 5 about to send best result to center 
rank 5 did not catch a best result 
Exit tag received on process 5 
process 5 waiting at barrier 
process 5 passed barrier 
rank 5, before deallocate 
rank 5, after deallocate 
rank 5, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 3 of 20 is on nia0049.scinet.local
About to start, 3 / 20!! 
rank 3 about to send best result to center 
rank 3 did not catch a best result 
Exit tag received on process 3 
process 3 waiting at barrier 
process 3 passed barrier 
rank 3, before deallocate 
rank 3, after deallocate 
rank 3, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 7 of 20 is on nia0284.scinet.local
About to start, 7 / 20!! 
rank 7 about to send best result to center 
rank 7 did not catch a best result 
Exit tag received on process 7 
process 7 waiting at barrier 
process 7 passed barrier 
rank 7, before deallocate 
rank 7, after deallocate 
rank 7, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 15 of 20 is on nia0784.scinet.local
About to start, 15 / 20!! 
rank 15 about to send best result to center 
rank 15 did not catch a best result 
Exit tag received on process 15 
process 15 waiting at barrier 
process 15 passed barrier 
rank 15, before deallocate 
rank 15, after deallocate 
rank 15, after MPI_Finalize() 
rank 17 sent best result, Bytes : 2152, refVal : 265
Exit tag received on process 17 
process 17 waiting at barrier 
process 17 passed barrier 
rank 17, before deallocate 
rank 17, after deallocate 
rank 17, after MPI_Finalize() 
argc: 3, threads: 20, filename: input/prob_4/600/0600_93 
The threading support level corresponds to that demanded.
Process 16 of 20 is on nia1176.scinet.local
About to start, 16 / 20!! 
MVC found so far has 272 elements...................thread 2
rank 16 updated refValueGlobalAbsolute to 272 || 272 
rank 16, cover size : 272 
rank 16, buffer size to be sent : 2180 
MVC found so far has 271 elements...................thread 8
rank 16 updated refValueGlobalAbsolute to 271 || 271 
rank 16, cover size : 271 
rank 16, buffer size to be sent : 2176 
MVC found so far has 270 elements..................thread 12
rank 16 updated refValueGlobalAbsolute to 270 || 270 
rank 16, cover size : 270 
rank 16, buffer size to be sent : 2172 
rank 16 about to send best result to center 
rank 16 put signal in inbox to retrieve a best result 
rank 16 sent best result, Bytes : 2172, refVal : 270
Exit tag received on process 16 
process 16 waiting at barrier 
process 16 passed barrier 
rank 16, before deallocate 
rank 16, after deallocate 
rank 16, after MPI_Finalize() 
Finishing run at: Tue Mar  2 18:15:29 EST 2021

scontrol show jobid 4925068
JobId=4925068 JobName=void_MPI_DLB
   UserId=pasr1602(3102120) GroupId=mlafond(6054778) MCS_label=N/A
   Priority=2294479 Nice=0 Account=def-mlafond QOS=normal
   JobState=COMPLETED Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:11:13 TimeLimit=02:00:00 TimeMin=N/A
   SubmitTime=2021-03-02T18:04:15 EligibleTime=2021-03-02T18:04:15
   AccrueTime=2021-03-02T18:04:15
   StartTime=2021-03-02T18:04:16 EndTime=2021-03-02T18:15:29 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-03-02T18:04:16
   Partition=compute AllocNode:Sid=nia-login01:159731
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia[0048-0049,0228,0284,0511,0552,0756,0784,1176-1177]
   BatchHost=nia0048
   NumNodes=10 NumCPUs=800 NumTasks=20 CPUs/Task=20 ReqB:S:C:T=0:0:*:*
   TRES=cpu=800,mem=1750G,node=10,billing=400
   Socks/Node=* NtasksPerN:B:S:C=2:0:*:* CoreSpec=*
   MinCPUsNode=40 MinMemoryNode=175G MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/m/mlafond/pasr1602/library/job.sh
   WorkDir=/gpfs/fs0/scratch/m/mlafond/pasr1602/library
   StdErr=/gpfs/fs0/scratch/m/mlafond/pasr1602/library/report/void_MPI_DLB-4925068.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/m/mlafond/pasr1602/library/report/void_MPI_DLB-4925068.out
   Power=
   MailUser=pasr1602@usherbrooke.ca MailType=BEGIN,END,FAIL,REQUEUE

sacct -j 4925068
       JobID    JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
4925068      void_MPI_+ def-mlafo+   00:11:13                        38:04.715 1-20:39:24      0:0 
4925068.bat+      batch def-mlafo+   00:11:13   1057404K     10280K  00:01.416  00:03.702      0:0 
4925068.ext+     extern def-mlafo+   00:11:13    138360K      1064K   00:00:00  00:00.007      0:0 
4925068.0         a.out def-mlafo+   00:10:57   1897252K    369724K  38:03.298 1-20:39:21      0:0 

kernel messages produced during job executions:
[Mar 2 17:53] fuse init (API version 7.22)
[Mar 2 18:04] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ec3
[  +0.011296] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ec5
[  +0.011229] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ece
[  +0.011179] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed0
[  +0.011163] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ec7
[  +0.011146] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed2
[  +0.011134] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ec8
[  +0.011126] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ec9
[  +0.011114] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eca
[  +0.011110] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed6
[  +0.011093] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ecb
[  +0.011080] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed8
[  +0.011064] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ecd
[  +0.011059] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9edb
[  +0.011037] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ecf
[  +0.011025] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9edd
[  +0.011010] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed1
[  +0.010984] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ede
[  +0.010971] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed3
[  +0.010953] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ee0
[  +0.010933] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed5
[  +0.010916] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed7
[  +0.010898] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ee4
[  +0.010881] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ee5
[  +0.010870] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ed9
[  +0.010852] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ee6
[  +0.010848] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eda
[  +0.010831] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9edc
[  +0.010811] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ee8
[  +0.010800] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9edf
[  +0.010794] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ee9
[  +0.010788] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ee1
[  +0.010788] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eea
[  +0.010783] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ee3
[  +0.430704] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9e9d
[  +0.010781] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9e9e
[  +0.010768] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9e9f
[  +0.010758] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ea3
[  +0.010744] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ea0
[  +0.010736] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ea4
[  +0.010735] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ea6
[  +0.010725] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ea5
[  +0.010719] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ea7
[  +0.010712] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ea8
[  +0.010709] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eaa
[  +0.010704] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eec
[  +0.010695] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ec1
[  +0.010686] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eee
[  +0.010671] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ec2
[  +0.010671] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eef
[  +0.010670] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eeb
[  +0.010673] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef1
[  +0.010671] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9eed
[  +0.010669] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef2
[  +0.010669] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef0
[  +0.010670] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef3
[  +0.010670] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef9
[  +0.010667] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef4
[  +0.010667] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9efa
[  +0.010670] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef5
[  +0.010667] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9efb
[  +0.010669] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef6
[  +0.010669] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9efc
[  +0.010673] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef7
[  +0.010672] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9efd
[  +0.010669] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9ef8
[  +0.010667] mlx5_core 0000:06:00.0: mlx5_core_get_rsc:63:(pid 0): Async event for bogus resource 0x9efe
